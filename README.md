The code from this repository has been moved to a new repository ([here](https://github.com/Stephanehk/Learning-OA-From-Prefs)), which contains code for both [Models of human preference for learning reward functions](https://arxiv.org/abs/2206.02231) and our follow-up paper [*Learning Optimal Advantage from Preferences and Mistaking it for Reward*](https://arxiv.org/abs/2310.02456).
